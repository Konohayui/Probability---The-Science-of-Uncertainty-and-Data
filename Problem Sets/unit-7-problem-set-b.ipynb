{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import math, random as rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Type of a Lightbulb\n",
    "#### Problem 1\n",
    "\n",
    "The lifetime of a type-A bulb is  exponentially distributed distributed with parameter $\\lambda$. The lifetime of a type-B bulb is exponentially distributed with parameter $\\mu$, where $\\mu>\\lambda>0$. You have a box of full of lightbulbs of the same type, and you would like to know whether they are of type A or B. Assume an a priori probability of $1/4$ that the box contains type-B lightbulbs.\n",
    "\n",
    "1. Assume the $\\mu\\geq3\\lambda$. You observe the value $t_1$ of the lifetime, $T_1$, of a lightbulb. A MAP decision rule decides that the lightbulb is of type A if and only if $t_1\\geq\\alpha$. Find $\\alpha$.\n",
    "\n",
    "Let $A$ and $B$ be the events that the box contains type A lightbulbs and type B lightbulbs respectively. A Map rule decides in favor of Type A if and only if\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(A\\,|\\,T_1=t_1)&\\geq\\mathbb{P}(B\\,|\\,T_1=t_1)\\\\\n",
    "\\frac{f_{T_1\\,|\\,A}(t_1\\,|\\,A)\\,\\mathbb{P}(A)}{f_{T_1}(t_1)}&\\geq\\frac{f_{T_1\\,|\\,B}(t_1\\,|\\,B)\\,\\mathbb{P}(B)}{f_{T_1}(t_1)}\\\\\n",
    "f_{T_1\\,|\\,A}(t_1\\,|\\,A)\\,\\mathbb{P}(A)&\\geq f_{T_1\\,|\\,B}(t_1\\,|\\,B)\\,\\mathbb{P}(B)\\\\\n",
    "\\frac{3\\lambda\\,e^{-\\lambda\\,t_1}}{4}&\\geq\\frac{\\mu\\,e^{-\\mu\\,t_1}}{4}\\\\\n",
    "\\frac{\\lambda}{\\mu}e^{(\\mu-\\lambda)t_1}&\\geq\\frac{1}{3}\\\\\n",
    "e^{(\\mu-\\lambda)t_1}&\\geq\\frac{\\mu}{3\\lambda}\\\\\n",
    "(\\mu-\\lambda)t_1&\\geq\\ln\\left(\\frac{\\mu}{3\\lambda}\\right)\\\\\n",
    "t_1&\\geq\\ln\\left(\\frac{\\mu}{3\\lambda}\\right)\\left(\\frac{1}{\\mu-\\lambda}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "So, $$\\alpha=\\ln\\left(\\frac{\\mu}{3\\lambda}\\right)\\left(\\frac{1}{\\mu-\\lambda}\\right)$$\n",
    "where $\\alpha\\geq0$ since $\\mu\\geq3\\lambda$.\n",
    "\n",
    "2. Assume again that $\\mu\\geq3\\lambda$. What is the probability of error of the MAP decision rule?\n",
    "\n",
    "An error occurs whenever the decision is different from the actual type of the lightbulb. Thus,\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\text{error})&=\\mathbb{P}(B\\,|\\,A)\\,\\mathbb{P}(A)+\\mathbb{P}(A\\,|\\,B)\\,\\mathbb{P}(B)\\\\\n",
    "&=\\mathbb{P}(T_1<\\alpha\\,|\\,A)\\,\\mathbb{P}(A)+\\mathbb{P}(T_1\\geq\\alpha\\,|\\,B)\\,\\mathbb{P}(B)\\\\\n",
    "&=\\left(1-e^{-\\lambda\\alpha}\\right)\\cdot\\frac{3}{4}+e^{-\\mu\\alpha}\\cdot\\frac{1}{4}\n",
    "\\end{align*}\n",
    "\n",
    "3. Assume that $\\lambda=3$ and $\\mu=4$. Find the LMS estimate of $T_2$, the lifetime of another lightbulb from the same box, based on observing $T_1=2$. Assume that conditioned on the bulb type, bulb lifetimes are independent.\n",
    "\n",
    "The LMS estimate of $T_2$ based on $T_1$ is \n",
    "\\begin{align*}\n",
    "E[T_2\\,|\\,T_1=t_1]&=E[T_2\\,|\\,T_1=t_1, A]\\,\\mathbb{P}(A\\,|\\,T_1=t_1)+E[T_2\\,|\\,T_1=t_1, B]\\,\\mathbb{P}(B\\,|\\,T_1=t_1)\\\\\n",
    "&=E[T_2\\,|\\,A]\\,\\mathbb{P}(A\\,|\\,T_1=t_1)+E[T_2\\,|\\,B]\\,\\mathbb{P}(B\\,|\\,T_1=t_1)\\\\\n",
    "&=\\left(\\frac{f_{T_1\\,|\\,A}(t_1\\,|\\,A)\\,\\mathbb{P}(A)}{f_{T_1}(t_1)}\\right)\\frac{1}{\\lambda}+\\left(\\frac{f_{T_1\\,|\\,B}(t_1\\,|\\,B)\\,\\mathbb{P}(B)}{f_{T_1}(t_1)}\\right)\\frac{1}{\\mu}\\\\\n",
    "&=\\left[\\frac{1}{\\lambda}\\frac{3}{4}\\lambda\\,e^{-\\lambda\\,t_1}+\\frac{1}{\\mu}\\frac{1}{4}\\mu\\,e^{-\\mu\\,t_1}\\right]\\Big/\\left[\\frac{3}{4}\\lambda\\,e^{-\\lambda\\,t_1}+\\frac{1}{4}\\mu\\,e^{-\\mu\\,t_1}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Pluging in $\\lambda=3$ and $\\mu=4$, we have $E[T_2\\,|\\,T_1=t_1]=0.328$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the Parameter of a Germetric Random Variable\n",
    "#### Problem 2\n",
    "\n",
    "We have $k$ coins. The probability of Heads is the same for each coin and is the realized value $q$ of a random variable $Q$ that is uniformly distributed on $[0, 1]$. We assume that conditioned on $Q=q$, all coin tosses are independent. Let $T_i$ be the number of tosses of the $i$th coin until that coin results in Heads for the first time, for $i=1, 2, \\dots, k$.($T_i$ includes the toss that results in the first Heads.)\n",
    "\n",
    "You may find the following integral usefule: for any non-negative integers $k$ and $m$,\n",
    "$$\\int_{0}^{1}q^k(1-q)^{m}\\,dq=\\frac{k!m!}{(k+m+1)!}$$\n",
    "\n",
    "1. Find the PMF of $T_1$.\n",
    "\n",
    "\\begin{align*}\n",
    "p_{T_1}(t)&=\\int_{0}^{1}p_{T_1\\,|\\,Q}(t\\,|\\,q)f_{Q}(q)\\,dq\\\\\n",
    "&=\\int_{0}^{1}(1-q)^{t-1}q\\,dq\\\\\n",
    "&=\\frac{(t-1)!}{(1+t-1+1)!}\\\\\n",
    "&=\\frac{1}{(t+1)t}\n",
    "\\end{align*}\n",
    "\n",
    "2. Find the LMS estimate of $Q$ based on the observed value, $t$, of $T_1$.\n",
    "\n",
    "\\begin{align*}\n",
    "E[Q\\,|\\,T_1=t]&=\\int_{0}^{1}q\\cdot\\,f_{Q\\,|\\,T_1}(q\\,|\\,T_1=t)\\,dq\\\\\n",
    "&=\\int_{0}^{1}\\frac{q\\cdot\\,p_{T_1\\,|\\,Q}(t\\,|\\,q)\\,f_Q(q)}{p_{T_1}(t)}\\,dq\\\\\n",
    "&=\\int_{0}^{1}t(t+1)(1-q)^{t-1}q^2\\,dq\\\\\n",
    "&=t(t+1)\\frac{2(t-1)!}{(t+2)!}\\\\\n",
    "&=\\frac{t}{t+2}\n",
    "\\end{align*}\n",
    "\n",
    "3. We flip each of the $k$ coins until they result in Heads for the first time. Continue the maximum a posteriori (MAP) estimate $\\hat{q}$ of $Q$ given the number of tosses needed, $T_1=t_1,\\dots,T_k=t_k$, for each coin.\n",
    "\n",
    "The posteriori of $Q$ given that $T_1=t_1,\\dots,T_k=t_k$ is \n",
    "\\begin{align*}\n",
    "f_{Q\\,|\\,T_1,\\dots\\,T_k}(q\\,|\\,T_1=t_1,\\dots\\,T_k=t_k)&=\\frac{f_Q(q)\\prod_{i=1}^{k}p_{T_i\\,|\\,Q}(t_i\\,|\\,q)}{\\int_{0}^{1}f_Q(q)\\prod_{i=1}^{k}p_{T_i\\,|\\,Q}(t_i\\,|\\,q)\\,dq}\\\\\n",
    "&=\\frac{q^k(1-q)^{\\sum_{i=1}^{k}(t_i-1)}}{\\int_{0}^{1}f_Q(q)\\prod_{i=1}^{k}p_{T_i\\,|\\,Q}(t_i\\,|\\,q)\\,dq}\n",
    "\\end{align*}\n",
    "\n",
    "To maximize the above expression, we take the derivative of the numerator and set it to zero. \n",
    "\\begin{align*}\n",
    "\\frac{d}{dq}\\left(q^k(1-q)^{\\sum_{i=1}^{k}(t_i-1)}\\right)&=0\\\\\n",
    "kq(1-q)^{\\sum_{i=1}^{k}(t_i-1)}-q\\sum_{i=1}^{k}(t_i-1)\\left((1-q)^{\\sum_{i=1}^{k}(t_i-1)-1}\\right)&=0\\\\\n",
    "k(1-q)-q\\left(\\sum_{i=1}^{k}t_i-k\\right)&=0\\\\\n",
    "q&=\\frac{k}{\\sum_{i=1}^{k}t_i}\n",
    "\\end{align*}\n",
    "\n",
    "Thus,\n",
    "$$\\hat{q}=\\frac{k}{\\sum_{i=1}^{k}t_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMS Estimation\n",
    "#### Problem 3\n",
    "\n",
    "Let $X=U+W$ with $E[U]=m$, $\\mathsf{Var}(U)=u$, $E[W]=0$, and $\\mathsf{Var}(W)=h$. Assume that $U$ and $W$ are independent.\n",
    "\n",
    "1. The LLMS estimator of $U$ based on $X$ is of the form $\\hat{U}=aX+b$. Find $a$ and $b$.\n",
    "\n",
    "$$\\hat{U}=E[U]+\\frac{\\mathsf{Cor}(U, X)}{\\mathsf{Var}(X)}(X-E[X])$$\n",
    "\n",
    "We need to find $\\mathsf{Cor}(U, X)$, $\\mathsf{Var}(X)$ and $E[X]$.\n",
    "\n",
    "\\begin{align*}\n",
    "E[X]&=E[U+W]\\\\\n",
    "&=E[U]+E[W]=m\\\\\n",
    "\\mathsf{Cor}(U, X)&=E[UX]-E[U]E[X]\\\\\n",
    "&=E[U(U+W)]-m^2\\\\\n",
    "&=E[U^2+UW]-m^2\\\\\n",
    "&=E[U^2]+E[UW]-m^2\\\\\n",
    "&=E[U^2]+E[U]E[W]-m^2\\\\\n",
    "&=E[U^2]-m^2\\\\\n",
    "&=E[U^2]-(E[U])^2\\\\\n",
    "&=\\mathsf{Var}(U)=u\\\\\n",
    "\\mathsf{Var}(X)&=\\mathsf{Var}(U+W)\\\\\n",
    "&=\\mathsf{Var}(U)+\\mathsf{Var}(W)\\\\\n",
    "&=u+h\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "$$\\hat{U}=m+\\frac{u}{u+h}(X-m)$$\n",
    "\n",
    "2. We now further assume that $U$ and $W$ are normal random variables and then construct $\\hat{U}_{LMS}$, the LMS estimator of $U$ based on $X$, under this additional assumption. Would $\\hat{U}_{LMS}$ be identical to $\\hat{U}$, the LLMS estimator developed without the additional normality assumption in Part 1?\n",
    "\n",
    "we know that the LMS estimator of $U$ based on $X$, under the normality assumption we have introduced, is linear in $X$. Therefore, it conincides with the LLMS estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMS Estimation with Random Sums\n",
    "#### Problem 4\n",
    "\n",
    "Let $N$ be a random variable with mean $E[N]=m$, and $\\mathsf{Var}(N)=v$; let $A_1, A_2,\\dots$ be a sequence of i.i.d. random variables, all independent of $N$, with mean $1$ and variance $1$; let $B_1,B_2,\\dots$ be another sequence of i.i.d. random variables,, all independent of $N$ and of $A_1,A_2,\\dots$ also with mean $1$ and variance $1$. Let $A=\\sum_{i=1}^{N}A_i$ and $B=\\sum_{i=1}^{N}B_i$.\n",
    "\n",
    "1. Find $E[AB]$ and $E[NA]$ using the law of iterated expectations.\n",
    "\n",
    "\\begin{align*}\n",
    "E[AB]&=E[E[AB]\\,|\\,N]\\\\\n",
    "&=E[E[A\\,|\\,N]\\cdot E[B\\,|\\,N]]\\\\\n",
    "&=E[NE[A_1]\\cdot NE[B_1]]\\\\\n",
    "&=E[N^2]\\\\\n",
    "&=\\mathsf{Var}(N)+(E[N])^2\\\\\n",
    "&=v+m^2\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "E[NA]&=E[E[NA]\\,|\\,N]\\\\\n",
    "&=E[NE[A]\\,|\\,N]\\\\\n",
    "&=E\\left[N\\sum_{i=1}^{N}E[A_1]\\,\\Big|\\,N\\right]\\\\\n",
    "&=E[N\\cdot N]\\\\\n",
    "&=\\mathsf{Var}(N)+(E[N])^2\\\\\n",
    "&=v+m^2\n",
    "\\end{align*}\n",
    "\n",
    "2. Let $\\hat{N}=c_1A+c_2$ be the LLMS estimator of $N$ given $A$. Find $c_1$ and $c_2$ in terms of $m$ and $v$.\n",
    "\n",
    "$$\\hat{N}=E[N]+\\frac{\\mathsf{Cor}(N, A)}{\\mathsf{Var}(A)}(A-E[A])$$\n",
    "\n",
    "\\begin{align*}\n",
    "E[A]&=E[E[A]\\,|\\,N]\\\\\n",
    "&=E\\left[\\sum_{i=1}^{N}E[A_i]\\,\\Big|\\,N\\right]\\\\\n",
    "&=E[NE[A_1]]=E[N]=m\\\\\n",
    "\\mathsf{Cor}(N, A)&=E[NA]-E[N]E[A]\\\\\n",
    "&=(v+m^2)-m\\cdot m\\\\\n",
    "&=v\\\\\n",
    "\\mathsf{Var}(A)&=E[\\mathsf{Var}(A\\,|\\,N)]+\\mathsf{Var}(E[A\\,|\\,N])\\\\\n",
    "&=E\\left[\\mathsf{Var}\\left(\\sum_{i=1}^{N}A_i\\,\\Big|\\,N\\right)\\right]+\\mathsf{Var}\\left(E\\left[\\sum_{i=1}^{N}A_i\\,\\Big|\\,N\\right]\\right)\\\\\n",
    "&=E[N\\cdot\\mathsf{Var}A_1]+\\mathsf{Var}(N\\cdot\\,E[A_1])\\\\\n",
    "&=E[N]+\\mathsf{Var}(N)=m+v\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Thus,\n",
    "$$\\hat{N}=m+\\frac{v}{m+v}(A-m)=\\frac{v}{m+v}A+\\frac{m^2}{m+v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Parameter of a Uniform Random Variable\n",
    "#### Problem 5\n",
    "\n",
    "The random variable $X$ is uniformly distributed over the interval $[\\theta,2\\theta]$. The parameter $\\theta$ is unknown and is modeled as the value of a continuous random variable $\\Theta$, uniformly distributed between zero and one.\n",
    "\n",
    "1. Given an onservation $x$ of $X$, find the posterior distribution of $\\Theta$.\n",
    "\n",
    "For $0\\leq x\\leq 1$ and $x/2\\leq\\theta\\leq x$\n",
    "\n",
    "The prior PDF of $\\Theta$ is \n",
    "$$f_{\\Theta}(\\theta)=\n",
    "\\begin{cases}\n",
    "1,&\\,\\,\\,\\text{if $0\\leq \\theta\\leq 1$}\\\\\n",
    "0,&\\,\\,\\,\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "and the conditional PDF of the observation $X$ is \n",
    "$$f_{X\\,|\\,\\Theta}(x\\,|\\,\\theta)=\n",
    "\\begin{cases}\n",
    "1/\\theta,&\\,\\,\\,\\text{if $\\theta\\leq x\\leq 2\\theta$}\\\\\n",
    "0,&\\,\\,\\,\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "For any $x\\in[0, 1]$ and for $\\theta\\in[x/2, x]$, the posterior PDF is\n",
    "\\begin{align*}\n",
    "f_{\\Theta\\,|\\,X}(\\theta\\,|\\,x)&=\\frac{f_{\\Theta}(\\theta)f_{X\\,|\\,\\Theta}(x\\,|\\,\\theta)}{\\int_{x/2}^{x}f_{\\Theta}(\\theta)f_{X\\,|\\,\\Theta}(x\\,|\\,\\theta)\\,d\\theta}\\\\\n",
    "&=\\frac{1/\\theta}{\\int_{x/2}^{x}1/\\theta\\,d\\theta}\\\\\n",
    "&=\\frac{1}{\\theta}\\frac{1}{\\ln(x)-\\ln(x/2)}\\\\\n",
    "&=\\frac{1}{\\theta\\ln(2)}\n",
    "\\end{align*}\n",
    "\n",
    "2. Find the MAP estimate of $\\Theta$ based on the observation $X=x$ and assuming that $0\\leq x\\leq1$.\n",
    "\n",
    "For $0\\leq x\\leq1$\n",
    "\n",
    "Since $x\\in[0, 1]$ and for $\\theta\\in[x/2, x]$ and the posterior PDF is \n",
    "$$f_{\\Theta\\,|\\,X}(\\theta\\,|\\,x)=\\frac{1}{\\theta\\ln(2)}$$\n",
    "$f_{\\Theta\\,|\\,X}(\\theta\\,|\\,x)$ decreases as $\\theta$ increases. The MAP estimate is equal to $x/2$.\n",
    "\n",
    "3. Find the LMS estimate of $\\Theta$ based on the observation $X=x$ and assuming that $0\\leq x\\leq1$.\n",
    "\n",
    "For $0\\leq x\\leq1$\n",
    "\n",
    "\\begin{align*}\n",
    "E[\\Theta\\,|\\,X=x]&=\\int_{x/2}^{x}\\theta\\cdot\\frac{1}{\\theta\\ln(2)}\\,d\\theta\\\\\n",
    "&=\\int_{x/2}^{x}\\frac{1}{\\ln(2)}\\,d\\theta\\\\\n",
    "&=\\frac{x}{2\\ln(2)}\n",
    "\\end{align*}\n",
    "\n",
    "4. Find the linear LMS estimate $\\hat{\\theta}_{LLMS}$ of $\\Theta$ based on the observation $X=x$. Specifically, $\\hat{\\theta}_{LLMS}$ is of the form $c_1+c_2x$. Find $c_1$ and $c_2$.\n",
    "\n",
    "$$\\hat{\\Theta}=E[\\Theta]+\\frac{\\mathsf{Cov}(\\Theta, X)}{\\mathsf{Var}(X)}(X-E[X])$$\n",
    "\n",
    "\\begin{align*}\n",
    "E[\\Theta]&=1/2\\\\\n",
    "E[\\Theta^2]&=\\mathsf{Var}(\\Theta)+(E[\\Theta])^2\\\\\n",
    "&=\\frac{1}{12}+\\frac{1}{4}=\\frac{1}{3}\\\\\n",
    "E[X]&=E[E[X\\,|\\,\\Theta]]\\\\\n",
    "&=E\\left[\\frac{3}{2}\\Theta\\right]\\\\\n",
    "&=\\frac{3}{4}\\\\\n",
    "\\mathsf{Var}(X)&=E[X^2]-(E[X])^2\\\\\n",
    "&=E[E[X^2\\,|\\,\\Theta]]-\\left(\\frac{3}{4}\\right)^2\\\\\n",
    "&=E\\left[\\frac{7}{3}\\Theta^2\\right]-\\left(\\frac{3}{4}\\right)^2\\\\\n",
    "&=\\frac{7}{9}-\\frac{9}{16}\\\\\n",
    "&=\\frac{31}{144}\\\\\n",
    "\\mathsf{Cov}(\\Theta,X)&=E[\\Theta X]-E[\\Theta]E[X]\\\\\n",
    "&=E[E[\\Theta\\,X\\,|\\,\\Theta]]-\\frac{1}{2}\\frac{3}{4}\\\\\n",
    "&=E[\\Theta\\,E[X\\,|\\,\\Theta]]-\\frac{3}{8}\\\\\n",
    "&=E\\left[\\frac{3}{2}\\Theta^2\\right]-\\frac{3}{8}\\\\\n",
    "&=\\frac{1}{8}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "$$\\hat{\\theta}_{LLMS}=\\frac{1}{2}+\\frac{1/8}{31/144}\\left(x-\\frac{3}{4}\\right)=\\frac{2}{31}+\\frac{18}{31}x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
